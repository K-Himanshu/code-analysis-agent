!git clone https://github.com/pallets/flask.git

import os
import json
import time
import requests
from textwrap import dedent
from google.colab import userdata

os.environ["GEMINI_API_KEY"] = userdata.get("API_KEY")



# ------------------------------
# CONFIG
# ------------------------------

MIN_SECONDS_BETWEEN_RUNS = 60
THROTTLE_CACHE_FILE = "/tmp/last_llm_call"

PROJECT_METADATA = {
    "purpose": "Web framework",
    "language": "Python",
    "audience": "Backend developers"
}

REPO_URL = "https://github.com/pallets/flask.git"
LOCAL_REPO_DIR = "flask"

# ------------------------------
# ERRORS
# ------------------------------

class LLMError(Exception):
    pass

# ------------------------------
# UTILS
# ------------------------------

def enforce_throttle():
    """Avoid calling LLM too frequently."""
    try:
        with open(THROTTLE_CACHE_FILE, "r") as f:
            last_call = float(f.read().strip())
        if time.time() - last_call < MIN_SECONDS_BETWEEN_RUNS:
            print("[agent] Throttle active, skipping LLM call.")
            return False
    except FileNotFoundError:
        pass

    with open(THROTTLE_CACHE_FILE, "w") as f:
        f.write(str(time.time()))
    return True

# ------------------------------
# CONTEXT COLLECTION
# ------------------------------

def clone_repo():
    if not os.path.exists(LOCAL_REPO_DIR):
        print("[agent] Cloning repository...")
        os.system(f"git clone {REPO_URL}")
    else:
        print("[agent] Repository already exists.")

def fetch_code_files(root_dir, extensions=(".py",)):
    files = []
    for root, _, filenames in os.walk(root_dir):
        for name in filenames:
            if name.endswith(extensions):
                path = os.path.join(root, name)
                try:
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                    files.append({
                        "filename": path,
                        "content": content
                    })
                except Exception:
                    pass
    return files

# ------------------------------
# CONTEXT REDUCTION
# ------------------------------

def summarize_code_for_prompt(files):
    max_files = 5
    max_lines_per_file = 200

    parts = []
    for f in files[:max_files]:
        lines = f["content"].splitlines()[:max_lines_per_file]
        parts.append(
            f"File: {f['filename']}\n" + "\n".join(lines)
        )
    return "\n\n".join(parts)

# ------------------------------
# PROMPT
# ------------------------------

def build_llm_prompt(code_summary, metadata):
    prompt = f"""
You are a senior software engineer.

Analyze the provided codebase and produce:

1. A concise summary of what the code does
2. Key design decisions
3. Potential risks, edge cases, or improvement areas

Project purpose: {metadata.get("purpose")}
Language: {metadata.get("language")}
Audience: {metadata.get("audience")}

Code (trimmed):
{code_summary}

Rules:
- Do not guess beyond the provided code
- Be factual and concise

Output format:

### Summary
...

### Design Notes
...

### Risks / Improvements
...
"""
    return dedent(prompt).strip()

# ------------------------------
# LLM CALL (UNCHANGED CORE IDEA)
# ------------------------------

def call_llm(prompt):
    if not enforce_throttle():
        raise LLMError("Throttled")

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise LLMError("GEMINI_API_KEY missing")

    url = (
        "https://generativelanguage.googleapis.com/v1/"
        f"models/gemini-2.5-flash:generateContent?key={api_key}"
    )

    payload = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ]
    }

    headers = {"Content-Type": "application/json"}

    resp = requests.post(url, json=payload, headers=headers)

    if resp.status_code == 429:
        raise LLMError("Rate limited")
    if resp.status_code >= 400:
        raise LLMError(f"LLM error {resp.status_code}: {resp.text}")

    data = resp.json()

    try:
        return data["candidates"][0]["content"]["parts"][0]["text"].strip()
    except Exception:
        raise LLMError("Unexpected LLM response")

# ------------------------------
# ACTION
# ------------------------------

def save_output(output, path="analysis.md"):
    with open(path, "w", encoding="utf-8") as f:
        f.write(output)

# ------------------------------
# MAIN AGENT LOOP
# ------------------------------

def main():
    print("[agent] Starting code analysis agent")

    clone_repo()

    files = fetch_code_files(LOCAL_REPO_DIR)
    if not files:
        print("[agent] No code files found.")
        return

    code_summary = summarize_code_for_prompt(files)
    prompt = build_llm_prompt(code_summary, PROJECT_METADATA)

    try:
        result = call_llm(prompt)
    except LLMError as e:
        print("[agent] LLM failed:", e)
        return

    save_output(result)
    print("[agent] Analysis complete. Saved to analysis.md")

# ------------------------------
# RUN
# ------------------------------

if __name__ == "__main__":
    main()
